{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from torch import nn\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, activation_size, n_dict_components, t_type=torch.float32, l1_coef=0.0):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.decoder = nn.Linear(n_dict_components, activation_size, bias=False)\n",
    "        # Initialize the decoder weights orthogonally\n",
    "        nn.init.orthogonal_(self.decoder.weight)\n",
    "        self.decoder = self.decoder.to(t_type)\n",
    "\n",
    "        self.encoder = nn.Sequential(nn.Linear(activation_size, n_dict_components).to(t_type), nn.ReLU())\n",
    "        self.l1_coef = l1_coef\n",
    "        self.activation_size = activation_size\n",
    "        self.n_dict_components = n_dict_components\n",
    "\n",
    "    def forward(self, x):\n",
    "        c = self.encoder(x)\n",
    "        # Apply unit norm constraint to the decoder weights\n",
    "        self.decoder.weight.data = nn.functional.normalize(self.decoder.weight.data, dim=0)\n",
    "\n",
    "        x_hat = self.decoder(c)\n",
    "        return x_hat, c\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "\n",
    "layer = 2\n",
    "setting = \"residual\"\n",
    "# setting = \"mlp\"\n",
    "# model_name = \"EleutherAI/pythia-1.4b-deduped\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "# filename = r'/root/sparse_coding/autoencoders4.pkl'\n",
    "# filename = r'/root/sparse_coding/auto_encoders.pkl'\n",
    "# filename = r'outputs/20230706-225337/auto_encoders_15.pkl'\n",
    "# filename = r'/root/pythia70m_layer2_residual_tied_.pkl'\n",
    "filename = f'/workspace/sparse_coding/outputs/20230729-101748/0/auto_encoders_2.pkl'\n",
    "# change this Julie\n",
    "\n",
    "if setting == \"residual\":\n",
    "    cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "elif setting == \"mlp\":\n",
    "    cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Load in the Pythia model w/ transformer lens\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "if(setting == \"residual\"):\n",
    "    neurons = model.cfg.d_model\n",
    "elif(setting == \"mlp\"):\n",
    "    neurons = model.cfg.d_mlp\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Load the pickle file\n",
    "with open(filename, 'rb') as file:\n",
    "    autoencoders = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_index = -1\n",
    "dictionaries = [autoencoder.decoder.weight.data.T for autoencoder in autoencoders[l1_index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([4096, 512])\n",
      "torch.Size([8192, 512])\n",
      "torch.Size([16384, 512])\n",
      "torch.Size([32768, 512])\n"
     ]
    }
   ],
   "source": [
    "for d in dictionaries:\n",
    "    print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(autoencoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_index = 2\n",
    "smaller_dict, larger_dict = dictionaries[dict_index], dictionaries[dict_index+1]\n",
    "smaller_auto_encoder, larger_auto_encoder = autoencoders[l1_index][dict_index], autoencoders[l1_index][dict_index+1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44579208 0.16210654 0.47205484 ... 0.18359646 0.3868705  0.17432681]\n"
     ]
    }
   ],
   "source": [
    "#Dictionary Comparison\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "smaller_dict_features, _ = smaller_dict.shape\n",
    "larger_dict_features, _ = larger_dict.shape\n",
    "larger_dict = larger_dict.to(device)\n",
    "# Hungary algorithm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "# Calculate all cosine similarities and store in a 2D array\n",
    "cos_sims = np.zeros((smaller_dict_features, larger_dict_features))\n",
    "for idx, vector in enumerate(smaller_dict):\n",
    "    cos_sims[idx] = torch.nn.functional.cosine_similarity(vector.to(device), larger_dict, dim=1).cpu().numpy()\n",
    "# Convert to a minimization problem\n",
    "cos_sims = 1 - cos_sims\n",
    "# Use the Hungarian algorithm to solve the assignment problem\n",
    "row_ind, col_ind = linear_sum_assignment(cos_sims)\n",
    "# Retrieve the max cosine similarities and corresponding indices\n",
    "max_cosine_similarities = 1 - cos_sims[row_ind, col_ind]\n",
    "\n",
    "print(max_cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the indices of the max cosine similarities in descending order\n",
    "max_indices = np.argsort(max_cosine_similarities)[::-1]\n",
    "max_cosine_similarities[max_indices][:20]\n",
    "print((max_cosine_similarities > .9).sum())\n",
    "# Plot histogram of max_cosine_similarities\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(max_cosine_similarities, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model activations & Dictionary Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Downnload dataset\n",
    "from datasets import Dataset, load_dataset\n",
    "dataset_name = \"NeelNanda/pile-10k\"\n",
    "token_amount=64\n",
    "dataset = load_dataset(dataset_name, split=\"train\").map(\n",
    "    lambda x: model.tokenizer(x['text']),\n",
    "    batched=True,\n",
    ").filter(\n",
    "    lambda x: len(x['input_ids']) > token_amount\n",
    ").map(\n",
    "    lambda x: {'input_ids': x['input_ids'][:token_amount]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1d8780d12f41faa8fb1c219df09c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we can use the model to get the activations\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "datapoints = dataset.num_rows\n",
    "batch_size = 32\n",
    "neuron_activations = torch.zeros((datapoints*token_amount, neurons))\n",
    "dictionary_activations = torch.zeros((datapoints*token_amount, smaller_dict_features))\n",
    "smaller_auto_encoder = smaller_auto_encoder.to(device)\n",
    "\n",
    "with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "    dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "    for i, batch in enumerate(tqdm(dl)):\n",
    "        _, cache = model.run_with_cache(batch.to(device))\n",
    "        batched_neuron_activations = rearrange(cache[cache_name], \"b s n -> (b s) n\" )\n",
    "        neuron_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_neuron_activations.cpu()\n",
    "        reconstruction, batched_dictionary_activations = smaller_auto_encoder(batched_neuron_activations)\n",
    "        dictionary_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_dictionary_activations.cpu()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron Sparsity per Feature by Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find corresponding neurons activations between pythia & autoencoder\n",
    "# Get the activations for the best dict features\n",
    "thresholds = [round(0.1*x,1) for x in range(11)]\n",
    "all_activated_neurons = []\n",
    "for threshold in thresholds:\n",
    "    current_activated_neurons = []\n",
    "    for x in range(20):\n",
    "        best_feature = max_indices[x]\n",
    "        best_feature_activations = dictionary_activations[:, best_feature]\n",
    "        # Sort the features by activation, get the indices\n",
    "        nonzero_indices = torch.argsort(best_feature_activations, descending=True)\n",
    "        sorted_indices = nonzero_indices[:10]\n",
    "        t = (neuron_activations[sorted_indices, :] > threshold)\n",
    "        # ( And across the first dim)\n",
    "        t = t.all(dim=0)\n",
    "        neurons_activated = t.sum()\n",
    "        current_activated_neurons.append(neurons_activated)\n",
    "        # print(f\"Feature {x} is active for {t.sum()} neurons\")\n",
    "    all_activated_neurons.append(current_activated_neurons)\n",
    "# Plot boxplot w/ plotly\n",
    "\n",
    "plt.boxplot(all_activated_neurons, labels=thresholds)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Number of neurons activated\")\n",
    "plt.title(\"Features/Neurons activated\")\n",
    "plt.ylim(0, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find number of non-zero activations for each feature in the dictionary\n",
    "nonzero_activations = dictionary_activations.count_nonzero(dim=0)\n",
    "# clip to 1\n",
    "nonzero_activations = nonzero_activations.clamp(max=1)\n",
    "# plot against the max cosine similarities\n",
    "# plt.hist(nonzero_activations.cpu().numpy(), bins=100)\n",
    "plt.scatter(max_cosine_similarities, nonzero_activations.cpu().numpy())\n",
    "# x-axis is the max cosine similarity\n",
    "# y-axis is the number of non-zero activations\n",
    "# now setting x-axis\n",
    "plt.xlabel(\"Max Cosine Similarity\")\n",
    "plt.ylabel(\"Number of Non-Zero Activations\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Activation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.activations import text_neuron_activations\n",
    "# Get the activations for the best dict features\n",
    "def get_feature_datapoints(feature_index, dictionary_activations, dataset, k=10, setting=\"max\"):\n",
    "    best_feature_activations = dictionary_activations[:, feature_index]\n",
    "    # Sort the features by activation, get the indices\n",
    "    if setting==\"max\":\n",
    "        found_indices = torch.argsort(best_feature_activations, descending=True)[:k]\n",
    "    elif setting==\"uniform\":\n",
    "        min_value = torch.min(best_feature_activations)\n",
    "        max_value = torch.max(best_feature_activations)\n",
    "\n",
    "        # Define the number of bins\n",
    "        num_bins = k\n",
    "\n",
    "        # Calculate the bin boundaries as linear interpolation between min and max\n",
    "        bin_boundaries = torch.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "        # Assign each activation to its respective bin\n",
    "        bins = torch.bucketize(best_feature_activations, bin_boundaries)\n",
    "\n",
    "        # Initialize a list to store the sampled indices\n",
    "        sampled_indices = []\n",
    "\n",
    "        # Sample from each bin\n",
    "        for bin_idx in torch.unique(bins):\n",
    "            # Get the indices corresponding to the current bin\n",
    "            bin_indices = torch.nonzero(bins == bin_idx, as_tuple=False).squeeze(dim=1)\n",
    "            \n",
    "            # Randomly sample from the current bin\n",
    "            sampled_indices.extend(np.random.choice(bin_indices, size=1, replace=False))\n",
    "\n",
    "        # Convert the sampled indices to a PyTorch tensor & reverse order\n",
    "        found_indices = torch.tensor(sampled_indices).long().flip(dims=[0])\n",
    "    else: # random\n",
    "        # get nonzero indices\n",
    "        nonzero_indices = torch.nonzero(best_feature_activations)[:, 0]\n",
    "        # shuffle\n",
    "        shuffled_indices = nonzero_indices[torch.randperm(nonzero_indices.shape[0])]\n",
    "        found_indices = shuffled_indices[:k]\n",
    "    datapoint_indices =[np.unravel_index(i, (datapoints, token_amount)) for i in found_indices]\n",
    "    text_list = []\n",
    "    full_text = []\n",
    "    token_list = []\n",
    "    full_token_list = []\n",
    "    for md, s_ind in datapoint_indices:\n",
    "        md = int(md)\n",
    "        s_ind = int(s_ind)\n",
    "        full_tok = torch.tensor(dataset[md][\"input_ids\"])\n",
    "        full_text.append(model.tokenizer.decode(full_tok))\n",
    "        tok = dataset[md][\"input_ids\"][:s_ind+1]\n",
    "        text = model.tokenizer.decode(tok)\n",
    "        text_list.append(text)\n",
    "        token_list.append(tok)\n",
    "        full_token_list.append(full_tok)\n",
    "    return text_list, full_text, token_list, full_token_list\n",
    "\n",
    "def get_neuron_activation(token, feature, model):\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(token.to(model.cfg.device))\n",
    "        neuron_act_batch = cache[cache_name]\n",
    "        _, act = smaller_auto_encoder(neuron_act_batch)\n",
    "    return act[0, :, feature].tolist()\n",
    "\n",
    "def ablate_text(text, feature, model, setting=\"plot\"):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    display_text_list = []\n",
    "    activation_list = []\n",
    "    for t in text:\n",
    "        # Convert text into tokens\n",
    "        if isinstance(t, str): # If the text is a list of tokens\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            tokens = model.to_tokens(t, prepend_bos=False)\n",
    "        else: # t equals tokens\n",
    "            tokens = t\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "        seq_size = tokens.shape[1]\n",
    "        if(seq_size == 1): # If the text is a single token, we can't ablate it\n",
    "            continue\n",
    "        original = get_neuron_activation(tokens, feature, model)[-1]\n",
    "        changed_activations = torch.zeros(seq_size, device=device).cpu()\n",
    "        for i in range(seq_size):\n",
    "            # Remove the i'th token from the input\n",
    "            ablated_tokens = torch.cat((tokens[:,:i], tokens[:,i+1:]), dim=1)\n",
    "            changed_activations[i] += get_neuron_activation(ablated_tokens, feature, model)[-1]\n",
    "        changed_activations -= original\n",
    "        display_text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "        activation_list += changed_activations.tolist() + [0.0]\n",
    "    activation_list = torch.tensor(activation_list).reshape(-1,1,1)\n",
    "    if setting == \"plot\":\n",
    "        return text_neuron_activations(tokens=display_text_list, activations=activation_list)\n",
    "    else:\n",
    "        return display_text_list, activation_list\n",
    "def visualize_text(text, feature, model, setting=\"plot\", max_activation = None):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    if isinstance(feature, int):\n",
    "        feature = [feature]\n",
    "    display_text_list = []\n",
    "    act_list = []\n",
    "    for t in text:\n",
    "        if isinstance(t, str): # If the text is a list of tokens\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            token = model.to_tokens(t, prepend_bos=False)\n",
    "        else: # t are tokens\n",
    "            token = t\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "        for f in feature:\n",
    "            display_text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            act_list += get_neuron_activation(token, f, model) + [0.0]\n",
    "    act_list = torch.tensor(act_list).reshape(-1,1,1)\n",
    "    if(max_activation is not None):\n",
    "        act_list = torch.clamp(act_list, max=max_activation)\n",
    "    return text_neuron_activations(tokens=display_text_list, activations=act_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablate the feature direction of the tokens\n",
    "# token_list is a list of tokens, convert to tensor of shape (batch_size, seq_len)\n",
    "from einops import rearrange\n",
    "def ablate_feature_direction(tokens, feature, model, autoencoder):\n",
    "    def mlp_ablation_hook(value, hook):\n",
    "        # Rearrange to fit autoencoder\n",
    "        int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "\n",
    "        # Run through the autoencoder\n",
    "        _, act = autoencoder(int_val)\n",
    "        feature_to_ablate = feature # TODO: bring this out of the function\n",
    "\n",
    "        # Subtract value with feature direction*act_of_feature\n",
    "        feature_direction = torch.outer(act[:, feature_to_ablate].squeeze(), autoencoder.decoder.weight[:, feature_to_ablate].squeeze())\n",
    "        batch, seq_len, hidden_size = value.shape\n",
    "        feature_direction = rearrange(feature_direction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "        value -= feature_direction\n",
    "        return value\n",
    "\n",
    "    return model.run_with_hooks(tokens, \n",
    "        fwd_hooks=[(\n",
    "            cache_name,\n",
    "            mlp_ablation_hook\n",
    "            )]\n",
    "        ) #returns all the logits \n",
    "\n",
    "\n",
    "\n",
    "def add_feature_direction(tokens, feature, model, autoencoder, amount):\n",
    "    def mlp_add_hook(value, hook):\n",
    "        # Rearrange to fit autoencoder\n",
    "        int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "\n",
    "        # Run through the autoencoder\n",
    "        _, act = autoencoder(int_val)\n",
    "        feature_to_ablate = feature # TODO: bring this out of the function\n",
    "\n",
    "        # Add value with feature direction*act_of_feature\n",
    "        #feature_direction = torch.outer(act[:, feature_to_ablate].squeeze(), autoencoder.decoder.weight[:, feature_to_ablate].squeeze())\n",
    "        feature_direction = autoencoder.decoder.weight[:, feature_to_ablate].squeeze()\n",
    "\n",
    "        # batch, seq_len, hidden_size = value.shape\n",
    "        # feature_direction = rearrange(feature_direction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "        value += amount * feature_direction\n",
    "        return value\n",
    "\n",
    "    return model.run_with_hooks(tokens, \n",
    "        fwd_hooks=[(\n",
    "            cache_name,\n",
    "            mlp_add_hook\n",
    "            )]\n",
    "        ) #returns all the logits \n",
    "\n",
    "\n",
    "\n",
    "def visualize_logit_diff(text, features=None, setting=\"true_tokens\", verbose=False):\n",
    "    features = best_feature\n",
    "\n",
    "    if features==None:\n",
    "        features = torch.tensor([best_feature])\n",
    "    if isinstance(features, int):\n",
    "        features = torch.tensor([features])\n",
    "    if isinstance(features, list):\n",
    "        features = torch.tensor(features)\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    text_list = []\n",
    "    logit_list = []\n",
    "    for t in text:\n",
    "        tokens = model.to_tokens(t, prepend_bos=False)\n",
    "        with torch.no_grad():\n",
    "            original_logits = model(tokens).log_softmax(-1).cpu()\n",
    "            ablated_logits = ablate_feature_direction(tokens, features, model, smaller_auto_encoder).log_softmax(-1).cpu()\n",
    "        diff_logits = ablated_logits  - original_logits# ablated > original -> negative diff\n",
    "        tokens = tokens.cpu()\n",
    "        if setting == \"true_tokens\":\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            gather_tokens = rearrange(tokens[:,1:], \"b s -> b s 1\") # TODO: verify this is correct\n",
    "            # Gather the logits for the true tokens\n",
    "            diff = rearrange(diff_logits[:, :-1].gather(-1,gather_tokens), \"b s n -> (b s n)\")\n",
    "        elif setting == \"max\":\n",
    "            # Negate the diff_logits to see which tokens have the largest effect on the neuron\n",
    "            val, ind = (-1*diff_logits).max(-1)\n",
    "            diff = rearrange(val[:, :-1], \"b s -> (b s)\")\n",
    "            diff*= -1 # Negate the values gathered\n",
    "            split_text = model.to_str_tokens(ind, prepend_bos=False)\n",
    "            gather_tokens = rearrange(ind[:,1:], \"1 s -> 1 s 1\")\n",
    "        split_text = split_text[1:] # Remove the first token since we're not predicting it\n",
    "        if(verbose):\n",
    "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            orig = rearrange(original_logits[:, :-1].gather(-1, gather_tokens), \"b s n -> (b s n)\")\n",
    "            ablated = rearrange(ablated_logits[:, :-1].gather(-1, gather_tokens), \"b s n -> (b s n)\")\n",
    "            logit_list += orig.tolist() + [0.0]\n",
    "            logit_list += ablated.tolist() + [0.0]\n",
    "        text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "        logit_list += diff.tolist() + [0.0]\n",
    "    logit_list = torch.tensor(logit_list).reshape(-1,1,1)\n",
    "    if verbose:\n",
    "        print(f\"Max & Min logit-diff: {logit_list.max().item():.2f} & {logit_list.min().item():.2f}\")\n",
    "    return text_neuron_activations(tokens=text_list, activations=logit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're no strangers to love, games but China,. you or me movies,, the all me or or to other her us.].\" or someone myself,... the when you another a the you us. to us\n",
      ". here ouryou in: you. sports,\n",
      "we're no strangers to love either objects young you when or us the him.\" them him you me and another them a the mism their people them their./ either you a\n",
      " enjoying each their their you someone or their the Evil ' people your you secrecy A  her Claire Brazil\n",
      "we're no strangers to love fict rather, your these and to.\n",
      ", a you him as,\" and an and any you or, or lone white you or you. or the you...\". or people, less-- him them. one us or our one such. them\n",
      "we're no strangers to love her. me old eng,\" automotive Leo one us her.. ' the the.'; YOU boy, these her blood sup ours you me a for and. and per. to.. your you. anything your Ab,. you and themselves and?\n",
      "we're no strangers to love without their who cocaine. my You a these Django us than with., seeing their the strangers you each,!\". people....\".. children your in their, another Children. Ant or men our Murray Car anyone about, a this the you\n",
      "we're no strangers to love people. them, Mc,' them The comrades, him but this your God a me or Jack anymore their,...\n",
      " you their making a or a Aaron ( them. employees their or us after a/ in her a any them to.\n",
      "we're no strangers to love or happiness any someone,! her their;, us, yet you her God nor us. a hearing the the anybody our Car or you or. rival friends a Dante, anyone, these these you\n",
      ". your you children us a them anymore any\n",
      "we're no strangers to love Tom us his when his what to.'\"!,.., New or so. me. your other who its our a in those.. their? you awe art some. his.. all bad looks or., by embodiments us secrets.\n",
      "we're no strangers to love, refugee Edward, a their duct, oldus, but or anything a, for her Felix their this. you.\" where Indian. patients but, their. her some or me, the to life that Arab you their me when their. or.\"\n",
      "we're no strangers to love this.\" her the,.\" to themod animals, again toys. or those to children West, their them and Fort any. the to.te, us., extended. another us or gas. his,.. the even to. agencies\n"
     ]
    }
   ],
   "source": [
    "#GENERATES TEXT\n",
    "\n",
    "#To do needs to include smth about ablating a feature\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch.nn.functional as F\n",
    "import random as random \n",
    "\n",
    "\n",
    "def generate_text(input_text, length, feature, amount):\n",
    "    generated_text = input_text\n",
    "    \n",
    "    input_ids = model.tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "    #token by token\n",
    "    for _ in range(length):  \n",
    "        # Get the model's predictions\n",
    "        with torch.no_grad():\n",
    "            #outputs = model(input_ids)\n",
    "            #print(outputs)\n",
    "\n",
    "            outputs = add_feature_direction(model.tokenizer.encode(input_text, return_tensors='pt'), feature, model, smaller_auto_encoder, amount)\t\n",
    "\n",
    "            \n",
    "            logits = outputs.log_softmax(-1).cpu()\n",
    "\n",
    "        logits = logits[0, -1, :]\n",
    "\n",
    "        \n",
    "\n",
    "        #teehee delete this here we could do a func on it \n",
    "        #logits = logits + random.randint(0, 100)/1 \n",
    "        \n",
    "        \n",
    "\n",
    "        # Sample the next token\n",
    "        next_token_id = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "\n",
    "        # temperature = 1.0  # Adjust this value to increase or decrease randomness\n",
    "        # next_token_id = torch.multivariate_normal(F.softmax(logits / temperature, dim=-1), num_samples=1)\n",
    "\n",
    "\n",
    "        # Decode the token and add it to the generated text\n",
    "        next_token = model.tokenizer.decode(next_token_id)\n",
    "        generated_text += next_token\n",
    "\n",
    "        # Update the input ids\n",
    "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=-1)  # The new token must be unsqueezed to have the correct shape\n",
    "        \n",
    "\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "#RUN THIS, RUN WITH ANY LOGITS, INPUT TEXT, LENGTH YOU LIKE \n",
    "input_text = \"we're no strangers to love\" \n",
    "\n",
    "#EDIT THIS TO ANY\n",
    "for amt in range(0, 10, 1):\n",
    "    print(generate_text(input_text, 50, 1605, amt/3))\n",
    "\n",
    "#hihi\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at feature 52, which has an MCS of 0.96 (w/ perfect MCS at 1.0). To get a feel for the feature, Let's plot the feature activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4080/194007354.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens_dataset = torch.tensor(tokens_dataset)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[  11],\n",
       "         [  21],\n",
       "         [  23],\n",
       "         [  34],\n",
       "         [  44],\n",
       "         [ 102],\n",
       "         [ 118],\n",
       "         [ 120],\n",
       "         [ 126],\n",
       "         [ 135],\n",
       "         [ 143],\n",
       "         [ 146],\n",
       "         [ 160],\n",
       "         [ 180],\n",
       "         [ 188],\n",
       "         [ 190],\n",
       "         [ 191],\n",
       "         [ 192],\n",
       "         [ 197],\n",
       "         [ 202],\n",
       "         [ 209],\n",
       "         [ 223],\n",
       "         [ 225],\n",
       "         [ 226],\n",
       "         [ 227],\n",
       "         [ 235],\n",
       "         [ 241],\n",
       "         [ 242],\n",
       "         [ 258],\n",
       "         [ 260],\n",
       "         [ 282],\n",
       "         [ 290],\n",
       "         [ 297],\n",
       "         [ 302],\n",
       "         [ 325],\n",
       "         [ 335],\n",
       "         [ 337],\n",
       "         [ 354],\n",
       "         [ 356],\n",
       "         [ 359],\n",
       "         [ 363],\n",
       "         [ 365],\n",
       "         [ 367],\n",
       "         [ 387],\n",
       "         [ 402],\n",
       "         [ 409],\n",
       "         [ 427],\n",
       "         [ 430],\n",
       "         [ 432],\n",
       "         [ 457],\n",
       "         [ 464],\n",
       "         [ 465],\n",
       "         [ 473],\n",
       "         [ 484],\n",
       "         [ 490],\n",
       "         [ 498],\n",
       "         [ 521],\n",
       "         [ 530],\n",
       "         [ 546],\n",
       "         [ 550],\n",
       "         [ 551],\n",
       "         [ 558],\n",
       "         [ 562],\n",
       "         [ 576],\n",
       "         [ 578],\n",
       "         [ 587],\n",
       "         [ 594],\n",
       "         [ 595],\n",
       "         [ 596],\n",
       "         [ 597],\n",
       "         [ 600],\n",
       "         [ 602],\n",
       "         [ 609],\n",
       "         [ 633],\n",
       "         [ 655],\n",
       "         [ 674],\n",
       "         [ 677],\n",
       "         [ 681],\n",
       "         [ 683],\n",
       "         [ 686],\n",
       "         [ 688],\n",
       "         [ 693],\n",
       "         [ 696],\n",
       "         [ 697],\n",
       "         [ 705],\n",
       "         [ 723],\n",
       "         [ 724],\n",
       "         [ 736],\n",
       "         [ 739],\n",
       "         [ 740],\n",
       "         [ 750],\n",
       "         [ 752],\n",
       "         [ 761],\n",
       "         [ 765],\n",
       "         [ 774],\n",
       "         [ 775],\n",
       "         [ 779],\n",
       "         [ 783],\n",
       "         [ 796],\n",
       "         [ 798],\n",
       "         [ 802],\n",
       "         [ 804],\n",
       "         [ 806],\n",
       "         [ 811],\n",
       "         [ 817],\n",
       "         [ 820],\n",
       "         [ 821],\n",
       "         [ 825],\n",
       "         [ 830],\n",
       "         [ 831],\n",
       "         [ 837],\n",
       "         [ 838],\n",
       "         [ 857],\n",
       "         [ 859],\n",
       "         [ 866],\n",
       "         [ 871],\n",
       "         [ 876],\n",
       "         [ 879],\n",
       "         [ 892],\n",
       "         [ 895],\n",
       "         [ 896],\n",
       "         [ 897],\n",
       "         [ 914],\n",
       "         [ 918],\n",
       "         [ 920],\n",
       "         [ 921],\n",
       "         [ 942],\n",
       "         [ 950],\n",
       "         [ 961],\n",
       "         [ 963],\n",
       "         [ 965],\n",
       "         [ 978],\n",
       "         [ 984],\n",
       "         [ 985],\n",
       "         [ 991],\n",
       "         [1019],\n",
       "         [1036],\n",
       "         [1044],\n",
       "         [1051],\n",
       "         [1067],\n",
       "         [1070],\n",
       "         [1079],\n",
       "         [1082],\n",
       "         [1094],\n",
       "         [1095],\n",
       "         [1097],\n",
       "         [1115],\n",
       "         [1116],\n",
       "         [1121],\n",
       "         [1124],\n",
       "         [1130],\n",
       "         [1151],\n",
       "         [1155],\n",
       "         [1159],\n",
       "         [1166],\n",
       "         [1176],\n",
       "         [1195],\n",
       "         [1196],\n",
       "         [1198],\n",
       "         [1218],\n",
       "         [1232],\n",
       "         [1236],\n",
       "         [1240],\n",
       "         [1242],\n",
       "         [1253],\n",
       "         [1256],\n",
       "         [1262],\n",
       "         [1263],\n",
       "         [1271],\n",
       "         [1293],\n",
       "         [1297],\n",
       "         [1302],\n",
       "         [1305],\n",
       "         [1312],\n",
       "         [1325],\n",
       "         [1335],\n",
       "         [1344],\n",
       "         [1346],\n",
       "         [1353],\n",
       "         [1354],\n",
       "         [1361],\n",
       "         [1363],\n",
       "         [1366],\n",
       "         [1383],\n",
       "         [1389],\n",
       "         [1390],\n",
       "         [1413],\n",
       "         [1429],\n",
       "         [1449],\n",
       "         [1453],\n",
       "         [1468],\n",
       "         [1477],\n",
       "         [1480],\n",
       "         [1495],\n",
       "         [1498],\n",
       "         [1509],\n",
       "         [1524],\n",
       "         [1535],\n",
       "         [1539],\n",
       "         [1545],\n",
       "         [1552],\n",
       "         [1554],\n",
       "         [1557],\n",
       "         [1558],\n",
       "         [1564],\n",
       "         [1566],\n",
       "         [1570],\n",
       "         [1571],\n",
       "         [1576],\n",
       "         [1578],\n",
       "         [1583],\n",
       "         [1588],\n",
       "         [1595],\n",
       "         [1628],\n",
       "         [1641],\n",
       "         [1648],\n",
       "         [1658],\n",
       "         [1659],\n",
       "         [1674],\n",
       "         [1681],\n",
       "         [1682],\n",
       "         [1683],\n",
       "         [1696],\n",
       "         [1721],\n",
       "         [1724],\n",
       "         [1736],\n",
       "         [1746],\n",
       "         [1766],\n",
       "         [1771],\n",
       "         [1773],\n",
       "         [1780],\n",
       "         [1782],\n",
       "         [1790],\n",
       "         [1791],\n",
       "         [1799],\n",
       "         [1826],\n",
       "         [1828],\n",
       "         [1856],\n",
       "         [1859],\n",
       "         [1876],\n",
       "         [1901],\n",
       "         [1911],\n",
       "         [1928],\n",
       "         [1930],\n",
       "         [1944],\n",
       "         [1949],\n",
       "         [1954],\n",
       "         [1957],\n",
       "         [1963],\n",
       "         [1969],\n",
       "         [1971],\n",
       "         [1988],\n",
       "         [1989],\n",
       "         [2028],\n",
       "         [2057],\n",
       "         [3498]]),\n",
       " torch.return_types.topk(\n",
       " values=tensor([7769., 1496., 1017.,  943.,  885.,  772.,  694.,  648.,  564.,  477.]),\n",
       " indices=tensor([ 773,  144, 1356,   95,  213,   15, 1841,    0,  632,   64])))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset[\"input_ids\"]\n",
    "tokens_dataset = [torch.tensor(x[\"input_ids\"]) for x in dataset]    \n",
    "tokens_dataset = torch.stack(tokens_dataset)\n",
    "tokens_dataset = rearrange(tokens_dataset, 't s-> (t s)')\n",
    "tokens_dataset.shape, dictionary_activations.shape\n",
    "size = 100000\n",
    "sub_tokens = tokens_dataset[:size]\n",
    "sub_activations = dictionary_activations[:size]\n",
    "import torch\n",
    "threshold = 2.0\n",
    "def get_unique_tokens_torch_efficient(dictionary_activations, tokens_dataset):\n",
    "    # Convert tokens_dataset to a torch tensor for efficient indexing\n",
    "    tokens_dataset = torch.tensor(tokens_dataset)\n",
    "    \n",
    "    # Initialize an empty dictionary to store counts for each feature\n",
    "    feature_counts = torch.zeros(dictionary_activations.size(1))\n",
    "\n",
    "    # Apply operations feature by feature\n",
    "    for feature_idx in range(dictionary_activations.size(1)):\n",
    "        # Get the activations for the current feature\n",
    "        feature_activations = dictionary_activations[:, feature_idx]\n",
    "        \n",
    "        # Find the indices of the non-zero activations\n",
    "        # nonzero_indices = torch.nonzero(feature_activations, as_tuple=True)[0]\n",
    "        nonzero_indices = torch.nonzero(feature_activations > threshold, as_tuple=True)[0]\n",
    "        \n",
    "        # Use these indices to find the corresponding tokens\n",
    "        feature_tokens = tokens_dataset[nonzero_indices]\n",
    "        \n",
    "        # Count the unique tokens\n",
    "        unique_tokens = torch.unique(feature_tokens)\n",
    "        feature_counts[feature_idx] = unique_tokens.numel()\n",
    "\n",
    "    return feature_counts\n",
    "\n",
    "feature_counts = get_unique_tokens_torch_efficient(sub_activations, sub_tokens)\n",
    "\n",
    "(feature_counts[max_indices.copy()] == 1).nonzero(), feature_counts[max_indices.copy()].topk(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Centric Viewpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-a75ee2e3-3c61\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.40.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-a75ee2e3-3c61\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"It\", \" is\", \" done\", \",\", \" and\", \" submitted\", \".\", \" You\", \" can\", \" play\", \" \\u201c\", \"Sur\", \"vival\", \" of\", \" the\", \" T\", \"ast\", \"iest\", \"\\u201d\", \" on\", \" Android\", \",\", \" and\", \" on\", \" the\", \" web\", \".\", \" Playing\", \" on\", \" the\", \" web\", \" works\", \",\", \" but\", \" you\", \" have\", \" to\", \" simulate\", \" multi\", \"-\", \"touch\", \" for\", \" table\", \" moving\", \" and\", \" that\", \" can\", \" be\", \" a\", \" bit\", \" confusing\", \".\", \"\\\\newline\", \"\\\\newline\", \"There\", \"\\u2019\", \"s\", \" a\", \" lot\", \" I\", \"\\u2019\", \"d\", \" like\", \" to\", \"\\n\", \"It\", \" is\", \" done\", \",\", \" and\", \" submitted\", \".\", \" You\", \" can\", \" play\", \" \\u201c\", \"Sur\", \"vival\", \" of\", \" the\", \" T\", \"ast\", \"iest\", \"\\u201d\", \" on\", \" Android\", \",\", \" and\", \" on\", \" the\", \" web\", \".\", \" Playing\", \" on\", \" the\", \" web\", \" works\", \",\", \" but\", \" you\", \" have\", \" to\", \" simulate\", \" multi\", \"-\", \"touch\", \" for\", \" table\", \" moving\", \" and\", \" that\", \" can\", \" be\", \" a\", \" bit\", \" confusing\", \".\", \"\\\\newline\", \"\\\\newline\", \"There\", \"\\u2019\", \"s\", \" a\", \" lot\", \" I\", \"\\u2019\", \"d\", \" like\", \" to\", \"\\n\", \"It\", \" is\", \" done\", \",\", \" and\", \" submitted\", \".\", \" You\", \" can\", \" play\", \" \\u201c\", \"Sur\", \"vival\", \" of\", \" the\", \" T\", \"ast\", \"iest\", \"\\u201d\", \" on\", \" Android\", \",\", \" and\", \" on\", \" the\", \" web\", \".\", \" Playing\", \" on\", \" the\", \" web\", \" works\", \",\", \" but\", \" you\", \" have\", \" to\", \" simulate\", \" multi\", \"-\", \"touch\", \" for\", \" table\", \" moving\", \" and\", \" that\", \" can\", \" be\", \" a\", \" bit\", \" confusing\", \".\", \"\\\\newline\", \"\\\\newline\", \"There\", \"\\u2019\", \"s\", \" a\", \" lot\", \" I\", \"\\u2019\", \"d\", \" like\", \" to\", \"\\n\", \"It\", \" is\", \" done\", \",\", \" and\", \" submitted\", \".\", \" You\", \" can\", \" play\", \" \\u201c\", \"Sur\", \"vival\", \" of\", \" the\", \" T\", \"ast\", \"iest\", \"\\u201d\", \" on\", \" Android\", \",\", \" and\", \" on\", \" the\", \" web\", \".\", \" Playing\", \" on\", \" the\", \" web\", \" works\", \",\", \" but\", \" you\", \" have\", \" to\", \" simulate\", \" multi\", \"-\", \"touch\", \" for\", \" table\", \" moving\", \" and\", \" that\", \" can\", \" be\", \" a\", \" bit\", \" confusing\", \".\", \"\\\\newline\", \"\\\\newline\", \"There\", \"\\u2019\", \"s\", \" a\", \" lot\", \" I\", \"\\u2019\", \"d\", \" like\", \" to\", \"\\n\", \"It\", \" is\", \" done\", \",\", \" and\", \" submitted\", \".\", \" You\", \" can\", \" play\", \" \\u201c\", \"Sur\", \"vival\", \" of\", \" the\", \" T\", \"ast\", \"iest\", \"\\u201d\", \" on\", \" Android\", \",\", \" and\", \" on\", \" the\", \" web\", \".\", \" Playing\", \" on\", \" the\", \" web\", \" works\", \",\", \" but\", \" you\", \" have\", \" to\", \" simulate\", \" multi\", \"-\", \"touch\", \" for\", \" table\", \" moving\", \" and\", \" that\", \" can\", \" be\", \" a\", \" bit\", \" confusing\", \".\", \"\\\\newline\", \"\\\\newline\", \"There\", \"\\u2019\", \"s\", \" a\", \" lot\", \" I\", \"\\u2019\", \"d\", \" like\", \" to\", \"\\n\", \"It\", \" is\", \" done\", \",\", \" and\", \" submitted\", \".\", \" You\", \" can\", \" play\", \" \\u201c\", \"Sur\", \"vival\", \" of\", \" the\", \" T\", \"ast\", \"iest\", \"\\u201d\", \" on\", \" Android\", \",\", \" and\", \" on\", \" the\", \" web\", \".\", \" Playing\", \" on\", \" the\", \" web\", \" works\", \",\", \" but\", \" you\", \" have\", \" to\", \" simulate\", \" multi\", \"-\", \"touch\", \" for\", \" table\", \" moving\", \" and\", \" that\", \" can\", \" be\", \" a\", \" bit\", \" confusing\", \".\", \"\\\\newline\", \"\\\\newline\", \"There\", \"\\u2019\", \"s\", \" a\", \" lot\", \" I\", \"\\u2019\", \"d\", \" like\", \" to\", \"\\n\", \"It\", \" is\", \" done\", \",\", \" and\", \" submitted\", \".\", \" You\", \" can\", \" play\", \" \\u201c\", \"Sur\", \"vival\", \" of\", \" the\", \" T\", \"ast\", \"iest\", \"\\u201d\", \" on\", \" Android\", \",\", \" and\", \" on\", \" the\", \" web\", \".\", \" Playing\", \" on\", \" the\", \" web\", \" works\", \",\", \" but\", \" you\", \" have\", \" to\", \" simulate\", \" multi\", \"-\", \"touch\", \" for\", \" table\", \" moving\", \" and\", \" that\", \" can\", \" be\", \" a\", \" bit\", \" confusing\", \".\", \"\\\\newline\", \"\\\\newline\", \"There\", \"\\u2019\", \"s\", \" a\", \" lot\", \" I\", \"\\u2019\", \"d\", \" like\", \" to\", \"\\n\", \"It\", \" is\", \" done\", \",\", \" and\", \" submitted\", \".\", \" You\", \" can\", \" play\", \" \\u201c\", \"Sur\", \"vival\", \" of\", \" the\", \" T\", \"ast\", \"iest\", \"\\u201d\", \" on\", \" Android\", \",\", \" and\", \" on\", \" the\", \" web\", \".\", \" Playing\", \" on\", \" the\", \" web\", \" works\", \",\", \" but\", \" you\", \" have\", \" to\", \" simulate\", \" multi\", \"-\", \"touch\", \" for\", \" table\", \" moving\", \" and\", \" that\", \" can\", \" be\", \" a\", \" bit\", \" confusing\", \".\", \"\\\\newline\", \"\\\\newline\", \"There\", \"\\u2019\", \"s\", \" a\", \" lot\", \" I\", \"\\u2019\", \"d\", \" like\", \" to\", \"\\n\", \"It\", \" is\", \" done\", \",\", \" and\", \" submitted\", \".\", \" You\", \" can\", \" play\", \" \\u201c\", \"Sur\", \"vival\", \" of\", \" the\", \" T\", \"ast\", \"iest\", \"\\u201d\", \" on\", \" Android\", \",\", \" and\", \" on\", \" the\", \" web\", \".\", \" Playing\", \" on\", \" the\", \" web\", \" works\", \",\", \" but\", \" you\", \" have\", \" to\", \" simulate\", \" multi\", \"-\", \"touch\", \" for\", \" table\", \" moving\", \" and\", \" that\", \" can\", \" be\", \" a\", \" bit\", \" confusing\", \".\", \"\\\\newline\", \"\\\\newline\", \"There\", \"\\u2019\", \"s\", \" a\", \" lot\", \" I\", \"\\u2019\", \"d\", \" like\", \" to\", \"\\n\", \"It\", \" is\", \" done\", \",\", \" and\", \" submitted\", \".\", \" You\", \" can\", \" play\", \" \\u201c\", \"Sur\", \"vival\", \" of\", \" the\", \" T\", \"ast\", \"iest\", \"\\u201d\", \" on\", \" Android\", \",\", \" and\", \" on\", \" the\", \" web\", \".\", \" Playing\", \" on\", \" the\", \" web\", \" works\", \",\", \" but\", \" you\", \" have\", \" to\", \" simulate\", \" multi\", \"-\", \"touch\", \" for\", \" table\", \" moving\", \" and\", \" that\", \" can\", \" be\", \" a\", \" bit\", \" confusing\", \".\", \"\\\\newline\", \"\\\\newline\", \"There\", \"\\u2019\", \"s\", \" a\", \" lot\", \" I\", \"\\u2019\", \"d\", \" like\", \" to\", \"\\n\"], \"activations\": [[[1.2873529195785522]], [[2.1439595222473145]], [[1.7218928337097168]], [[2.3341875076293945]], [[3.2349276542663574]], [[0.0]], [[0.0]], [[2.466434955596924]], [[0.6784025430679321]], [[0.02570369839668274]], [[0.0]], [[0.4865427017211914]], [[0.45611512660980225]], [[0.8257083892822266]], [[1.4587644338607788]], [[0.0]], [[0.0]], [[0.0]], [[0.05219706892967224]], [[0.0]], [[0.47613489627838135]], [[0.7017876505851746]], [[0.1210746169090271]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7648806571960449]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.08035863935947418]], [[0.0]], [[0.03547613322734833]], [[0.0]], [[0.0]], [[0.0]], [[0.15083375573158264]], [[0.0]], [[0.0]], [[0.0]], [[0.5322976112365723]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4819619059562683]], [[0.0]], [[0.0]], [[1.3943241834640503]], [[0.0]], [[2.9192705154418945]], [[0.0]], [[2.3034181594848633]], [[0.0]], [[3.916940689086914]], [[1.2855740785598755]], [[3.24068021774292]], [[0.8061014413833618]], [[0.7947918176651001]], [[0.6675348877906799]], [[0.0]], [[0.2165350615978241]], [[2.0650784969329834]], [[1.6944705247879028]], [[1.7955361604690552]], [[1.6036314964294434]], [[1.2751437425613403]], [[0.0]], [[1.031831979751587]], [[0.4129413664340973]], [[0.04898303747177124]], [[0.5490963459014893]], [[0.18870040774345398]], [[0.0]], [[0.11453408002853394]], [[0.6464442014694214]], [[0.23376598954200745]], [[0.10427215695381165]], [[0.30464842915534973]], [[0.1661452054977417]], [[0.02256503701210022]], [[0.0]], [[0.3954213559627533]], [[0.17177000641822815]], [[0.06539911031723022]], [[0.27895721793174744]], [[0.31177833676338196]], [[0.0245000422000885]], [[0.0]], [[0.0]], [[0.0]], [[0.11689114570617676]], [[0.0]], [[0.20099881291389465]], [[0.0]], [[0.135901540517807]], [[0.002401873469352722]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.040654927492141724]], [[0.0]], [[0.14776575565338135]], [[0.0]], [[0.050267159938812256]], [[0.0]], [[0.0]], [[0.0512295663356781]], [[0.2497216761112213]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.13283389806747437]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.3298429250717163]], [[0.4376610517501831]], [[0.3297732472419739]], [[0.0]], [[0.0]], [[0.0]], [[0.21370983123779297]], [[0.3484022617340088]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.33045661449432373]], [[0.09965759515762329]], [[0.14603057503700256]], [[0.148818701505661]], [[0.0]], [[0.0]], [[0.058448731899261475]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.13159120082855225]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.07071587443351746]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.3277710676193237]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.3348705768585205]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.992471933364868]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.717863917350769]], [[1.514780879020691]], [[1.1147576570510864]], [[0.8249655961990356]], [[0.763613224029541]], [[0.0]], [[0.1524953544139862]], [[0.0]], [[0.4245573580265045]], [[0.2037648856639862]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.2431151568889618]], [[0.14721763134002686]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.027901679277420044]], [[0.1553569734096527]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.15961018204689026]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.4517800509929657]], [[0.07562491297721863]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03533965349197388]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.2071973979473114]], [[0.0]], [[0.0]], [[0.45416319370269775]], [[0.84654700756073]], [[1.091673493385315]], [[1.4699158668518066]], [[1.4371294975280762]], [[0.0]], [[0.8386423587799072]], [[0.9059404134750366]], [[0.49982213973999023]], [[0.29856055974960327]], [[0.23932820558547974]], [[0.06957137584686279]], [[0.0]], [[0.0]], [[0.04527994990348816]], [[0.22916102409362793]], [[0.4633913040161133]], [[0.3517756462097168]], [[0.07512766122817993]], [[0.0]], [[0.2222568392753601]], [[0.11692383885383606]], [[0.16470038890838623]], [[0.4800611138343811]], [[0.08681139349937439]], [[0.03956899046897888]], [[0.20709899067878723]], [[0.48263275623321533]], [[0.5443992018699646]], [[0.125424325466156]], [[0.0]], [[0.161940336227417]], [[0.33730918169021606]], [[0.49767976999282837]], [[0.5243017077445984]], [[0.3454812169075012]], [[0.28067833185195923]], [[0.23343926668167114]], [[0.16819030046463013]], [[0.34495627880096436]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.13648968935012817]], [[0.0]], [[0.0]], [[0.0]], [[0.018487930297851562]], [[0.15378031134605408]], [[0.08228540420532227]], [[0.2031850516796112]], [[0.38637447357177734]], [[0.06523394584655762]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.0514707565307617]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.26101353764533997]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.2882511615753174]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.8636665940284729]], [[1.052443504333496]], [[0.9730560183525085]], [[0.4258708357810974]], [[0.5796181559562683]], [[0.0]], [[0.5093991160392761]], [[0.37339383363723755]], [[0.2878962755203247]], [[0.04161469638347626]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.27020663022994995]], [[0.03163105249404907]], [[0.13021637499332428]], [[0.16785700619220734]], [[0.0]], [[0.07410068809986115]], [[0.26293641328811646]], [[0.24170194566249847]], [[0.0]], [[0.0]], [[0.1426636427640915]], [[0.0]], [[0.0]], [[0.1781420260667801]], [[0.293491005897522]], [[0.2127285748720169]], [[0.20050080120563507]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.030294135212898254]], [[0.0]], [[0.08218313753604889]], [[0.2871302366256714]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.9664673209190369]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0649866983294487]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.04630383104085922]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.7367005348205566]], [[1.3817803859710693]], [[1.2014329433441162]], [[0.9602710008621216]], [[0.7829294204711914]], [[0.550623893737793]], [[0.0]], [[0.46713221073150635]], [[0.4584106206893921]], [[0.024317964911460876]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.5159329175949097]], [[0.8980727195739746]], [[0.5370688438415527]], [[0.14036062359809875]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.2047041356563568]], [[0.0]], [[0.1871882677078247]], [[0.083712637424469]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0018213093280792236]], [[0.05806460976600647]], [[0.0]], [[0.10421693325042725]], [[0.0]], [[0.0]], [[0.006019771099090576]], [[0.30588048696517944]], [[0.4096797704696655]], [[0.7436094284057617]], [[0.6158355474472046]], [[0.5640193819999695]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fdd4cb40110>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Go through datapoints & see if the features that activate on them make sense.\n",
    "d_point = 3\n",
    "text = tokens_dataset[d_point]\n",
    "data_ind, sequence_pos = np.unravel_index(d_point, (datapoints, token_amount))\n",
    "feature_val, feature_ind = dictionary_activations[d_point].topk(10)\n",
    "data_ind = int(data_ind)\n",
    "sequence_pos = int(sequence_pos)\n",
    "full_tok = torch.tensor(dataset[data_ind][\"input_ids\"])\n",
    "full_text = []\n",
    "full_text.append(model.tokenizer.decode(full_tok))\n",
    "visualize_text(full_text, feature_ind, model, setting=\"plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  11],\n",
       "         [  21],\n",
       "         [  23],\n",
       "         [  34],\n",
       "         [  44],\n",
       "         [ 102],\n",
       "         [ 118],\n",
       "         [ 120],\n",
       "         [ 126],\n",
       "         [ 135],\n",
       "         [ 143],\n",
       "         [ 146],\n",
       "         [ 160],\n",
       "         [ 180],\n",
       "         [ 188],\n",
       "         [ 190],\n",
       "         [ 191],\n",
       "         [ 192],\n",
       "         [ 197],\n",
       "         [ 202],\n",
       "         [ 209],\n",
       "         [ 223],\n",
       "         [ 225],\n",
       "         [ 226],\n",
       "         [ 227],\n",
       "         [ 235],\n",
       "         [ 241],\n",
       "         [ 242],\n",
       "         [ 258],\n",
       "         [ 260],\n",
       "         [ 282],\n",
       "         [ 290],\n",
       "         [ 297],\n",
       "         [ 302],\n",
       "         [ 325],\n",
       "         [ 335],\n",
       "         [ 337],\n",
       "         [ 354],\n",
       "         [ 356],\n",
       "         [ 359],\n",
       "         [ 363],\n",
       "         [ 365],\n",
       "         [ 367],\n",
       "         [ 387],\n",
       "         [ 402],\n",
       "         [ 409],\n",
       "         [ 427],\n",
       "         [ 430],\n",
       "         [ 432],\n",
       "         [ 457],\n",
       "         [ 464],\n",
       "         [ 465],\n",
       "         [ 473],\n",
       "         [ 484],\n",
       "         [ 490],\n",
       "         [ 498],\n",
       "         [ 521],\n",
       "         [ 530],\n",
       "         [ 546],\n",
       "         [ 550],\n",
       "         [ 551],\n",
       "         [ 558],\n",
       "         [ 562],\n",
       "         [ 576],\n",
       "         [ 578],\n",
       "         [ 587],\n",
       "         [ 594],\n",
       "         [ 595],\n",
       "         [ 596],\n",
       "         [ 597],\n",
       "         [ 600],\n",
       "         [ 602],\n",
       "         [ 609],\n",
       "         [ 633],\n",
       "         [ 655],\n",
       "         [ 674],\n",
       "         [ 677],\n",
       "         [ 681],\n",
       "         [ 683],\n",
       "         [ 686],\n",
       "         [ 688],\n",
       "         [ 693],\n",
       "         [ 696],\n",
       "         [ 697],\n",
       "         [ 705],\n",
       "         [ 723],\n",
       "         [ 724],\n",
       "         [ 736],\n",
       "         [ 739],\n",
       "         [ 740],\n",
       "         [ 750],\n",
       "         [ 752],\n",
       "         [ 761],\n",
       "         [ 765],\n",
       "         [ 774],\n",
       "         [ 775],\n",
       "         [ 779],\n",
       "         [ 783],\n",
       "         [ 796],\n",
       "         [ 798],\n",
       "         [ 802],\n",
       "         [ 804],\n",
       "         [ 806],\n",
       "         [ 811],\n",
       "         [ 817],\n",
       "         [ 820],\n",
       "         [ 821],\n",
       "         [ 825],\n",
       "         [ 830],\n",
       "         [ 831],\n",
       "         [ 837],\n",
       "         [ 838],\n",
       "         [ 857],\n",
       "         [ 859],\n",
       "         [ 866],\n",
       "         [ 871],\n",
       "         [ 876],\n",
       "         [ 879],\n",
       "         [ 892],\n",
       "         [ 895],\n",
       "         [ 896],\n",
       "         [ 897],\n",
       "         [ 914],\n",
       "         [ 918],\n",
       "         [ 920],\n",
       "         [ 921],\n",
       "         [ 942],\n",
       "         [ 950],\n",
       "         [ 961],\n",
       "         [ 963],\n",
       "         [ 965],\n",
       "         [ 978],\n",
       "         [ 984],\n",
       "         [ 985],\n",
       "         [ 991],\n",
       "         [1019],\n",
       "         [1036],\n",
       "         [1044],\n",
       "         [1051],\n",
       "         [1067],\n",
       "         [1070],\n",
       "         [1079],\n",
       "         [1082],\n",
       "         [1094],\n",
       "         [1095],\n",
       "         [1097],\n",
       "         [1115],\n",
       "         [1116],\n",
       "         [1121],\n",
       "         [1124],\n",
       "         [1130],\n",
       "         [1151],\n",
       "         [1155],\n",
       "         [1159],\n",
       "         [1166],\n",
       "         [1176],\n",
       "         [1195],\n",
       "         [1196],\n",
       "         [1198],\n",
       "         [1218],\n",
       "         [1232],\n",
       "         [1236],\n",
       "         [1240],\n",
       "         [1242],\n",
       "         [1253],\n",
       "         [1256],\n",
       "         [1262],\n",
       "         [1263],\n",
       "         [1271],\n",
       "         [1293],\n",
       "         [1297],\n",
       "         [1302],\n",
       "         [1305],\n",
       "         [1312],\n",
       "         [1325],\n",
       "         [1335],\n",
       "         [1344],\n",
       "         [1346],\n",
       "         [1353],\n",
       "         [1354],\n",
       "         [1361],\n",
       "         [1363],\n",
       "         [1366],\n",
       "         [1383],\n",
       "         [1389],\n",
       "         [1390],\n",
       "         [1413],\n",
       "         [1429],\n",
       "         [1449],\n",
       "         [1453],\n",
       "         [1468],\n",
       "         [1477],\n",
       "         [1480],\n",
       "         [1495],\n",
       "         [1498],\n",
       "         [1509],\n",
       "         [1524],\n",
       "         [1535],\n",
       "         [1539],\n",
       "         [1545],\n",
       "         [1552],\n",
       "         [1554],\n",
       "         [1557],\n",
       "         [1558],\n",
       "         [1564],\n",
       "         [1566],\n",
       "         [1570],\n",
       "         [1571],\n",
       "         [1576],\n",
       "         [1578],\n",
       "         [1583],\n",
       "         [1588],\n",
       "         [1595],\n",
       "         [1628],\n",
       "         [1641],\n",
       "         [1648],\n",
       "         [1658],\n",
       "         [1659],\n",
       "         [1674],\n",
       "         [1681],\n",
       "         [1682],\n",
       "         [1683],\n",
       "         [1696],\n",
       "         [1721],\n",
       "         [1724],\n",
       "         [1736],\n",
       "         [1746],\n",
       "         [1766],\n",
       "         [1771],\n",
       "         [1773],\n",
       "         [1780],\n",
       "         [1782],\n",
       "         [1790],\n",
       "         [1791],\n",
       "         [1799],\n",
       "         [1826],\n",
       "         [1828],\n",
       "         [1856],\n",
       "         [1859],\n",
       "         [1876],\n",
       "         [1901],\n",
       "         [1911],\n",
       "         [1928],\n",
       "         [1930],\n",
       "         [1944],\n",
       "         [1949],\n",
       "         [1954],\n",
       "         [1957],\n",
       "         [1963],\n",
       "         [1969],\n",
       "         [1971],\n",
       "         [1988],\n",
       "         [1989],\n",
       "         [2028],\n",
       "         [2057],\n",
       "         [3498]]),\n",
       " torch.return_types.topk(\n",
       " values=tensor([7769., 1496., 1017.,  943.,  885.,  772.,  694.,  648.,  564.,  477.,\n",
       "          455.,  391.,  380.,  348.,  299.,  289.,  281.,  272.,  271.,  266.,\n",
       "          264.,  264.,  263.,  258.,  252.,  251.,  244.,  244.,  238.,  218.,\n",
       "          211.,  207.,  200.,  189.,  178.,  177.,  176.,  174.,  171.,  167.,\n",
       "          166.,  164.,  147.,  141.,  139.,  134.,  133.,  133.,  132.,  132.]),\n",
       " indices=tensor([ 773,  144, 1356,   95,  213,   15, 1841,    0,  632,   64,   92,  100,\n",
       "           14, 1184,  113, 1104,   13,  715,   94,  208,   66,  149,   59,  538,\n",
       "           47,  523,  195,  314,  569,  131,   67,  910,  342,  339,  574,  315,\n",
       "         1126,  446,  749,  181,   63,  431,   76,  403,  229,  637, 1314,  289,\n",
       "         1451,  503])))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(feature_counts[max_indices.copy()] == 1).nonzero(), feature_counts[max_indices.copy()].topk(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(858, 3.4884), (306, 1.9837), (1746, 1.4728), (1005, 1.4562), (1540, 1.3844), (87, 1.3085), (1174, 1.2541), (545, 1.2519), (2098, 1.0869), (159, 0.952), (3908, 0.9453), (3449, 0.8956), (3857, 0.8952), (573, 0.8869), (3419, 0.8648), (1455, 0.853), (521, 0.8338), (3572, 0.7994), (2302, 0.7915), (365, 0.7692)]\n"
     ]
    }
   ],
   "source": [
    "# sentences = ['he was rude and mean', \"evil bully\", \" illegal vandalism\", \" ruthless kill\"]\n",
    "# whitelist = dict()\n",
    "# for t in sentences:\n",
    "#     split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "#     token = model.to_tokens(t, prepend_bos=False)\n",
    "#     _, cache = model.run_with_cache(token.to(model.cfg.device))\n",
    "#     neuron_act_batch = cache[cache_name]\n",
    "#     _, act = smaller_auto_encoder(neuron_act_batch)\n",
    "#     features = act[0, -1, :].topk(50).indices\n",
    "#     values = act[0, -1, :].topk(50).values\n",
    "#     blacklist = [1717, 1828, 1291, 1998, 1284, 1415, 1820, 4, 1672, 401, 1752, 2028, 1154, 1971, 1746, 1454, 636, 1287, 1536, 1220, 509, 1156, 1602, 1375, 1129, 413, 1817, 1599, 1724, 1951, 648, 741, 962, 1270, 949, 453, 57, 900, 1394, 1619, 322, 814, 2043, 182, 1474, 1469, 2031, 300, 1576, 1225, 2041, 706, 1614, 177, 814, 133, 1515, 1886, 31]\n",
    "#     for feature, value in zip(features, values):\n",
    "#         if feature not in blacklist:\n",
    "#             if feature in whitelist:\n",
    "#                 whitelist[feature.item()] += value.item()\n",
    "#             else:\n",
    "#                 whitelist[feature.item()] = round(value.item(), 4)\n",
    "# print(sorted(list(whitelist.items()), key=lambda x:x[1], reverse=True))\n",
    "\n",
    "\n",
    "#Adam new version\n",
    "sentences = ['son of a bitch', \"what the fuck\", \"you are a slut\", \" shithead\"]\n",
    "outlier_filter = ['\\newline...\\newline...']\n",
    "# blacklist = [1717, 1828, 1291, 1998, 1284, 1415, 1820, 4, 1672, 401, 1752, 2028, 1154, 1971, 1746, 1454, 636, 1287, 1536, 1220, 509, 1156, 1602, 1375, 1129, 413, 1817, 1599, 1724, 1951, 648, 741, 962, 1270, 949, 453, 57, 900, 1394, 1619, 322, 814, 2043, 182, 1474, 1469, 2031, 300, 1576, 1225, 2041, 706, 1614, 177, 814, 133, 1515, 1886, 31]\n",
    "blacklist = []\n",
    "whitelist = dict()\n",
    "for t in sentences:\n",
    "    split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "    token = model.to_tokens(t, prepend_bos=False)\n",
    "    _, cache = model.run_with_cache(token.to(model.cfg.device))\n",
    "    neuron_act_batch = cache[cache_name]\n",
    "    _, act = smaller_auto_encoder(neuron_act_batch)\n",
    "    features = act[0, -1, :].topk(100).indices\n",
    "    values = act[0, -1, :].topk(100).values\n",
    "    \n",
    "    for feature, value in zip(features, values):\n",
    "        if feature not in blacklist:\n",
    "            if feature in whitelist:\n",
    "                whitelist[feature.item()] += value.item()\n",
    "            else:\n",
    "                whitelist[feature.item()] = round(value.item(), 4)\n",
    "candidates = sorted(list(whitelist.items()), key=lambda x:x[1], reverse=True)\n",
    "final_output = list()\n",
    "for feature, value in candidates:\n",
    "    split_text = model.to_str_tokens(outlier_filter, prepend_bos=False)\n",
    "    token = model.to_tokens(outlier_filter, prepend_bos=False)\n",
    "    _, cache = model.run_with_cache(token.to(model.cfg.device))\n",
    "    neuron_act_batch = cache[cache_name]\n",
    "    _, act = smaller_auto_encoder(neuron_act_batch)\n",
    "    if act[0, 0, feature] < 2:\n",
    "        final_output.append((feature, value))\n",
    "print(final_output[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_index= 2940\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AutoEncoder' object has no attribute 'encoder_bias'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeat_index=\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_feature)\n\u001b[1;32m      4\u001b[0m best_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1605\u001b[39m\u001b[38;5;66;03m#feature num 10\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43msmaller_auto_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_bias\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[best_feature])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax feature index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMCS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_cosine_similarities[best_feature]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/sparse_coding/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AutoEncoder' object has no attribute 'encoder_bias'"
     ]
    }
   ],
   "source": [
    "N = 11\n",
    "best_feature = int(max_indices[N])\n",
    "print(\"feat_index=\", best_feature)\n",
    "best_feature = 1605#feature num 10\n",
    "\n",
    "print(\"bias:\", smaller_auto_encoder.encoder_bias.detach().cpu().numpy()[best_feature])\n",
    "print(f\"Max feature index: {N}\")\n",
    "print(f\"MCS: {max_cosine_similarities[best_feature]}\")\n",
    "# text_list, full_text, token_list, full_token_list = get_feature_datapoints(best_feature, dictionary_activations, dataset, setting=\"uniform\")\n",
    "text_list, full_text, token_list, full_token_list = get_feature_datapoints(best_feature, dictionary_activations, dataset, setting=\"max\")\n",
    "visualize_text(full_text, best_feature, model, setting=\"plot\")\n",
    "# visualize_text(text_list, best_feature, model, setting=\"plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_logit_diff(full_text, best_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    bad_ind = (model.W_U.norm(dim=0) > 20)\n",
    "    feature_direction = smaller_dict[best_feature].to(device)\n",
    "    # residual_direction = torch.matmul(feature_direction, model.W_out[layer]) # Add bias\n",
    "    # feature_direction = model.ln_final(feature_direction)\n",
    "    logits = torch.matmul(feature_direction, model.W_U).cpu()\n",
    "    # logits = model.W_U(residual_direction).cpu()\n",
    "# Don't include bad indices\n",
    "logits[bad_ind] = -1000\n",
    "topk_values, topk_indices = torch.topk(logits, 20)\n",
    "top_text = model.to_str_tokens(topk_indices)\n",
    "print(f\"{top_text}\")\n",
    "print(topk_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablate_text(text_list, best_feature, model, setting=\"plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of top-k examples, just random samples from non-zero values\n",
    "text_list, full_text, _, _ = get_feature_datapoints(best_feature, dictionary_activations, dataset, setting=\"uniform\", k=10)\n",
    "visualize_text(full_text, best_feature, model, setting=\"plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_text(text_list, best_feature, model, setting=\"plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_all_tokens_and_get_feature_activation(model, minimal_activating_example, feature, setting=\"prepend\"):\n",
    "    tokens = model.to_tokens(minimal_activating_example, prepend_bos=False)\n",
    "\n",
    "    # Run through every number up to vocab size\n",
    "    vocab_size = model.cfg.d_vocab\n",
    "    batch_size = 256 # Define your desired batch size\n",
    "\n",
    "    dollar_feature_activations = torch.zeros(vocab_size)\n",
    "    for start in range(0, vocab_size, batch_size):\n",
    "        end = min(start + batch_size, vocab_size)\n",
    "\n",
    "        token_prep = torch.arange(start, end).to(device)\n",
    "        token_prep = token_prep.unsqueeze(1)  # Add a dimension for concatenation\n",
    "\n",
    "        # 1. Prepend to the tokens\n",
    "        if setting == \"prepend\":\n",
    "            tokens_catted = torch.cat((token_prep, tokens.repeat(end - start, 1)), dim=1).long()\n",
    "        elif setting == \"append\":\n",
    "            tokens_catted = torch.cat((tokens.repeat(end - start, 1), token_prep), dim=1).long()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown setting: {setting}\")\n",
    "\n",
    "        # 2. Run through the model\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(tokens_catted.to(device))\n",
    "            neuron_act_batch = cache[cache_name]\n",
    "            _, act = smaller_auto_encoder(neuron_act_batch)\n",
    "\n",
    "        # 3. Get the feature\n",
    "        dollar_feature_activations[start:end] = act[:, -1, feature].cpu().squeeze()\n",
    "\n",
    "    k = 20\n",
    "    k_increasing_val, k_increasing_ind = dollar_feature_activations.topk(k)\n",
    "    k_decreasing_val, k_decreasing_ind = dollar_feature_activations.topk(k, largest=False)\n",
    "    if(setting == \"prepend\"):\n",
    "        print(f\"[token]{minimal_activating_example}\")\n",
    "    elif(setting == \"append\"):\n",
    "        print(f\"{minimal_activating_example}[token]\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown setting: {setting}\")\n",
    "    # Print indices converted to tokens\n",
    "    print(f\"Top-{k} increasing: {model.to_str_tokens(k_increasing_ind)}\")\n",
    "    # Print values\n",
    "    print(f\"Top-{k} increasing: {[f'{val:.2f}' for val in k_increasing_val]}\")\n",
    "    print(f\"Top-{k} decreasing: {model.to_str_tokens(k_decreasing_ind)}\")\n",
    "    print(f\"Top-{k} decreasing: {[f'{val:.2f}' for val in k_decreasing_val]}\")\n",
    "    print(f\"Number of 0 activations: {torch.sum(dollar_feature_activations == 0)}\")\n",
    "    if(setting == \"prepend\"):\n",
    "        best_text = \"\".join(model.to_str_tokens(dollar_feature_activations.argmax()) + [minimal_activating_example])\n",
    "    else:\n",
    "        best_text = \"\".join([minimal_activating_example] + model.to_str_tokens(dollar_feature_activations.argmax()))\n",
    "    return best_text\n",
    "\n",
    "best_text = \"\"\n",
    "for x in range(3):\n",
    "    # best_text = prepend_all_tokens_and_get_feature_activation(model, best_text, best_feature, setting=\"prepend\")\n",
    "    best_text = prepend_all_tokens_and_get_feature_activation(model, best_text, best_feature, setting=\"append\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \" for all $\", best_feature, setting=\"prepend\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \" tree\", best_feature, setting=\"prepend\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \" tree\", best_feature, setting=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_text = \",\"\n",
    "for x in range(3):\n",
    "    best_text = prepend_all_tokens_and_get_feature_activation(model, best_text, best_feature, setting=\"prepend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepend_all_tokens_and_get_feature_activation(model, \" x\", best_feature, setting=\"prepend\")\n",
    "prepend_all_tokens_and_get_feature_activation(model, \" let\", best_feature, setting=\"append\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \" tree\", best_feature, setting=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \" $\",\n",
    "    \" for $\",\n",
    "    \" integral $\",\n",
    "    \" hey $\",\n",
    "    \" integral for $\",\n",
    "    \" sold for $\",\n",
    "    \" for all $\",\n",
    "    \" sold for all $\",\n",
    "    \" profit for all $\",\n",
    "    \" \\n for all $\",\n",
    "    \" {] for all $\",\n",
    "]\n",
    "text_list = [\n",
    "    \" The bugs and trees went up. The bugs\",\n",
    "    \" The bugs, bees, and trees went up. The bugs, bees,\",\n",
    "    \" The bugs, bees, and trees went up. The bugs, bees, and trees went down. The bugs, bees,\",\n",
    "    \" He counted 1, 2, 3!\",\n",
    "    \" He counted 1, 2, 3!\",\n",
    "]\n",
    "text_list = [\n",
    "    \"The picture is as clear as black and white.\",\n",
    "    \"Could you pass the salt and pepper, please?\",\n",
    "    \"This job is our bread and butter, we can't afford to lose it.\",\n",
    "    \"We need to consider the pros and cons before making a decision.\",\n",
    "    \"It's a process of trial and error until we find the right solution.\",\n",
    "    \"When you flip a coin, do you choose heads or tails?\",\n",
    "    \"We have to act fast, it's now or never.\",\n",
    "    \"The concert was a mix of jazz and rock and roll.\",\n",
    "    \"In England, it's traditional to have fish and chips on Fridays.\",\n",
    "    \"The bride and groom looked absolutely stunning.\",\n",
    "]\n",
    "\n",
    "visualize_text(text_list, best_feature, model, setting=\"plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablate_text(text_list, best_feature, model, setting=\"plot\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the first token, this doesn't fit within the hypothesis, though may be OOD. A check (for the future) would be to constrain by the model's next word prediction (and maybe do direct soft prompts)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing to the Neuron Basis\n",
    "Does this feature net us anything over using the normal neuron basis? If this is only learning a monosemantic feature, then that's pretty lame! \n",
    "\n",
    "We can first check how many neurons activate above a threshold for the top-10 feature activating examples (ie a neuron must activate above threshold for all 10 examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neurons that fire for this feature\n",
    "# Find corresponding neurons activations between pythia & autoencoder\n",
    "# Get the activations for the best dict features\n",
    "len_threshold = 11\n",
    "thresholds = [round(0.1*x,1) for x in range(len_threshold)]\n",
    "current_activated_neurons = np.zeros(len_threshold)\n",
    "for idx, threshold in enumerate(thresholds):\n",
    "    best_feature_activations = dictionary_activations[:, best_feature]\n",
    "    # Sort the features by activation, get the indices\n",
    "    nonzero_indices = torch.argsort(best_feature_activations, descending=True)\n",
    "    sorted_indices = nonzero_indices[:10]\n",
    "    t = (neuron_activations[sorted_indices, :] > threshold)\n",
    "    # And across the first dim)\n",
    "    t = t.all(dim=0)\n",
    "    neurons_activated = t.sum()\n",
    "    current_activated_neurons[idx] = neurons_activated\n",
    "    print(f\"Threshold: {threshold}, Neurons activated: {neurons_activated}\")\n",
    "# Plot boxplot w/ plotly\n",
    "plt.scatter(thresholds, current_activated_neurons)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Number of neurons activated\")\n",
    "plt.title(\"Features/Neurons activated\")\n",
    "# plt.ylim(0, 10)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably it's a lot of neurons for above 0, but then goes to 3 for 0.5. However, we don't really know the statistics of neuron activations. Maybe some neuron's entire range is very tiny? So, we can see how many neurons are above a threshold determined by that neuron's quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "len_threshold = 11\n",
    "thresholds = [round(0.1 * x, 1) for x in range(len_threshold)]\n",
    "thresholds[-1] = 0.99\n",
    "current_activated_neurons = np.zeros(len_threshold)\n",
    "\n",
    "# Find corresponding neurons activations between pythia & autoencoder\n",
    "best_feature_activations = dictionary_activations[:, best_feature]\n",
    "\n",
    "# Sort the features by activation and get the indices\n",
    "nonzero_indices = torch.argsort(best_feature_activations, descending=True)\n",
    "sorted_indices = nonzero_indices[:11]\n",
    "\n",
    "neuron_test = neuron_activations[:50000, :]\n",
    "neuron_test = neuron_test.to(device)\n",
    "thresholds_t = torch.tensor(thresholds).to(device)\n",
    "neuron_activations = neuron_activations.to(device)\n",
    "# above_quantile = neuron_test[above_zero_indices].quantile(0.5, dim=0)\n",
    "\n",
    "# Calculate the quantiles for each neuron's activation range\n",
    "quantiles = torch.quantile(neuron_test, thresholds_t, dim=0)\n",
    "\n",
    "for idx, threshold in enumerate(quantiles):\n",
    "    # Determine which neurons activate above their specific quantile\n",
    "    above_threshold_mask = neuron_activations[sorted_indices, :] > threshold.unsqueeze(0)\n",
    "    neurons_activated = above_threshold_mask.float().mean(dim=0).count_nonzero()\n",
    "    current_activated_neurons[idx] = neurons_activated\n",
    "    print(f\"Quantile: {thresholds[idx]}, Neurons activated: {neurons_activated}\")\n",
    "\n",
    "# Plot scatter plot\n",
    "plt.scatter(thresholds, current_activated_neurons)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Quantile\")\n",
    "plt.ylabel(\"Number of neurons activated\")\n",
    "plt.title(\"Features/Neurons activated\")\n",
    "# plt.ylim(0, 200)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, 80 neurons are activating in above their top 1% of activations.\n",
    "\n",
    "Another way is to look at the decoder of the dictionary & see the weights effect on neurons since it's just a linear layer. Additionally, I'll multiply by the max-activation of that feature to show the scale of the effect on the neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check features non-zero weights in decoder\n",
    "# Plot a histogram of the weights\n",
    "max_activation = dictionary_activations[:, best_feature].max()\n",
    "weights = smaller_dict[best_feature]\n",
    "plt.hist(weights, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(weights).topk(20, largest=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a few datapoints to indicate somewhere between 3 & 80 neurons, maybe several hundred depending on how you interpret the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logit lens\n",
    "# Multiply feature direction by W_out[layer]\n",
    "# Then by W_U\n",
    "for n in range(52, 53):\n",
    "    b_feature = max_indices[n]\n",
    "    with torch.no_grad():\n",
    "        feature_direction = smaller_dict[b_feature].to(device)\n",
    "        residual_direction = torch.matmul(feature_direction, model.W_out[layer]) # Add bias\n",
    "        # residual_direction = model.ln_final(residual_direction)\n",
    "        logits = torch.matmul(residual_direction, model.W_U).cpu()\n",
    "    topk_values, topk_indices = torch.topk(logits, 10)\n",
    "    top_text = model.to_str_tokens(topk_indices)\n",
    "    print(f\"Feature {n}: {top_text}\")\n",
    "# print(topk_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import circuitsvis\n",
    "# text = full_text\n",
    "# features = best_feature\n",
    "\n",
    "# if features==None:\n",
    "#     features = torch.tensor([best_feature])\n",
    "# if isinstance(features, int):\n",
    "#     features = torch.tensor([features])\n",
    "# if isinstance(features, list):\n",
    "#     features = torch.tensor(features)\n",
    "# if isinstance(text, str):\n",
    "#     text = [text]\n",
    "# text_list = []\n",
    "# logit_list = []\n",
    "# for t in text:\n",
    "#     tokens = model.to_tokens(t, prepend_bos=False)\n",
    "#     with torch.no_grad():\n",
    "#         original_logits = model(tokens).log_softmax(-1).cpu()\n",
    "#         ablated_logits = ablate_feature_direction(tokens, features, model, smaller_auto_encoder).log_softmax(-1).cpu()\n",
    "#     # diff_logits = ablated_logits  - original_logits# ablated > original -> negative diff\n",
    "#     diff_logits =   original_logits - ablated_logits# ablated > original -> negative diff\n",
    "#     tokens = tokens.cpu()\n",
    "#     split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "#     gather_tokens = rearrange(tokens[:,1:], \"b s -> b s 1\") # TODO: verify this is correct\n",
    "#     # Gather the logits for the true tokens\n",
    "#     true_log_probs = rearrange(diff_logits[:, :-1].gather(-1,gather_tokens), \"b s n -> (b s n)\")\n",
    "#     break\n",
    "# # Add an extra dim for the batch\n",
    "# diff_logits = diff_logits[0]\n",
    "# tokens = tokens[0]\n",
    "# print(diff_logits.shape, tokens.shape)\n",
    "# # circuitsvis.logits.token_log_probs(token_indices=tokens, top_k=10, log_probs=original_logits, to_string=model.to_single_str_token)\n",
    "# circuitsvis.logits.token_log_probs(token_indices=tokens, top_k=10, log_probs=diff_logits, to_string=model.to_single_str_token)\n",
    "# # circuitsvis.logits.token_log_probs(token_indices=tokens, top_k=10, log_probs=ablated_logits, to_string=model.to_single_str_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#Plot a histogram\n",
    "import matplotlib.pyplot as plt\n",
    "for x in range(10):\n",
    "    max_elements = (dictionary_activations[:, max_indices[x]]>0.01)\n",
    "    plt.hist(dictionary_activations[max_elements, max_indices[x]], bins=20)\n",
    "    plt.title('Histogram of Activations for Dictionary Element ' + str(x))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#Plot a histogram\n",
    "import matplotlib.pyplot as plt\n",
    "for x in range(10):\n",
    "    print((dictionary_activations[:, max_indices[x]] > 0.01).sum().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
